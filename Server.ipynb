{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Server.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bH1TBjcM-W",
        "colab_type": "code",
        "outputId": "4fa6e783-da27-4df1-807e-76239d26414c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# All codes which are done for setting up the server (Setting up the server environment, Installation of model dependencies and etc)\n",
        "\n",
        "#########################################################################\n",
        "# OCR part\n",
        "#########################################################################\n",
        "!sudo apt install tesseract-ocr\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Object Detection part\n",
        "#########################################################################\n",
        "# install dependencies\n",
        "def install():\n",
        "  !pip install Flask\n",
        "  !pip install flask-ngrok\n",
        "  !pip install -U torch torchvision cython\n",
        "  !pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "  !git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "  !pip install -e detectron2_repo\n",
        "\n",
        "install()\n",
        "\n",
        "# After install\n",
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup\n",
        "# Setup detectron2 logger\n",
        "import torch, torchvision\n",
        "torch.__version__\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import OrderedDict\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# make pretrained model object\n",
        "predictor = None\n",
        "cfg = None\n",
        "def setup():\n",
        "  global cfg\n",
        "  global predictor\n",
        "  cfg = get_cfg()\n",
        "  cfg.merge_from_file(\"./detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "  # Find a model from detectron2's model zoo. You can either use the https://dl.fbaipublicfiles.... url, or use the following shorthand\n",
        "  cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "\n",
        "setup()\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# TTS part\n",
        "#########################################################################\n",
        "# def install_tts():\n",
        "#   # Clone git repo\n",
        "#   !git clone https://github.com/CorentinJ/Real-Time-Voice-Cloning.git\n",
        "#   %cd Real-Time-Voice-Cloning/\n",
        "#   # Install dependencies\n",
        "#   !pip install -q -r requirements.txt\n",
        "#   !apt-get install -qq libportaudio2\n",
        "#   # Download dataset\n",
        "#   !gdown https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc\n",
        "#   !unzip pretrained.zip\n",
        "#   !gdown https://drive.google.com/uc?id=1eZNxBYyf9Rg7i7wwSpIyD91uaxkGqsPl\n",
        "\n",
        "# install_tts()\n",
        "  \n",
        "# # Code for recording audio from the browser\n",
        "\n",
        "# from IPython.display import Javascript\n",
        "# from google.colab import output\n",
        "# from base64 import b64decode\n",
        "# import IPython\n",
        "# import uuid\n",
        "# from google.colab import output\n",
        "# from IPython.display import Audio\n",
        "# from IPython.utils import io\n",
        "# from synthesizer.inference import Synthesizer\n",
        "# from encoder import inference as encoder\n",
        "# from vocoder import inference as vocoder\n",
        "# from pathlib import Path\n",
        "# import numpy as np\n",
        "# import librosa\n",
        "# synthesizer = None\n",
        "# def setup_tts():\n",
        "#   global synthesizer\n",
        "#   print(\"inside set up\")\n",
        "#   encoder_weights = Path(\"encoder/saved_models/pretrained.pt\")\n",
        "#   vocoder_weights = Path(\"vocoder/saved_models/pretrained/pretrained.pt\")\n",
        "#   syn_dir = Path(\"synthesizer/saved_models/logs-pretrained/taco_pretrained\")\n",
        "#   encoder.load_model(encoder_weights)\n",
        "#   synthesizer = Synthesizer(syn_dir)\n",
        "#   vocoder.load_model(vocoder_weights)\n",
        "# setup_tts()\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "# Image Captioning part\n",
        "#########################################################################\n",
        "#실제로 할 때는 url 관련된 부분 지우고, IMAGE_FILE 변경, replace 경로 변경\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tempfile import mkstemp\n",
        "from shutil import move\n",
        "from os import fdopen, remove\n",
        "\n",
        "\n",
        "def replace(file_path, pattern, subst):\n",
        "    #Create temp file\n",
        "    fh, abs_path = mkstemp()\n",
        "    with fdopen(fh,'w') as new_file:\n",
        "        with open(file_path) as old_file:\n",
        "            for line in old_file:\n",
        "                new_file.write(line.replace(pattern, subst))\n",
        "            old_file.close()\n",
        "        new_file.close()\n",
        "    #Remove original file\n",
        "    remove(file_path)\n",
        "    #Move new file\n",
        "    move(abs_path, file_path)\n",
        "\n",
        "def modify_files():\n",
        "    OLD_CHECKPOINT_FILE = \"/content/Pretrained-Show-and-Tell-model/model.ckpt-2000000\"\n",
        "    NEW_CHECKPOINT_FILE = \"/content/Pretrained-Show-and-Tell-model/model2.ckpt-2000000\"\n",
        "\n",
        "    import tensorflow as tf\n",
        "    vars_to_rename = {\n",
        "        \"lstm/basic_lstm_cell/weights\": \"lstm/basic_lstm_cell/kernel\",\n",
        "        \"lstm/basic_lstm_cell/biases\": \"lstm/basic_lstm_cell/bias\",\n",
        "    }\n",
        "    new_checkpoint_vars = {}\n",
        "    reader = tf.train.NewCheckpointReader(OLD_CHECKPOINT_FILE)\n",
        "    for old_name in reader.get_variable_to_shape_map():\n",
        "        if old_name in vars_to_rename:\n",
        "            new_name = vars_to_rename[old_name]\n",
        "    else:\n",
        "        new_name = old_name\n",
        "    new_checkpoint_vars[new_name] = tf.Variable(reader.get_tensor(old_name))\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver(new_checkpoint_vars)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        saver.save(sess, NEW_CHECKPOINT_FILE)\n",
        "\n",
        "def setup_img_captioning():\n",
        "    # Pre-trained model git repo.\n",
        "    %cd /content\n",
        "    !git clone https://github.com/KranthiGV/Pretrained-Show-and-Tell-model.git\n",
        "\n",
        "    # im2txt git repo.\n",
        "    !git init img_caption\n",
        "    %cd img_caption\n",
        "    %ls .\n",
        "    !git config core.sparseCheckout true\n",
        "    !git remote add -f origin https://github.com/tensorflow/models.git\n",
        "    !echo \"research/im2txt\" >> .git/info/sparse-checkout\n",
        "    !git pull origin master\n",
        "\n",
        "    %cd /content\n",
        "    # download packages, libraries\n",
        "    !pip install virtualenv\n",
        "    !virtualenv venv\n",
        "    !virtualenv -p /usr/bin/python2.7 --distribute temp-python\n",
        "    # os.system(\"virtualenv -p /usr/bin/python2.7 --distribute temp-python\")\n",
        "    !source temp-python/bin/activate\n",
        "    # !python -c 'import sys; print(sys.version_info[:])'\n",
        "    !sudo apt autoremove\n",
        "    !sudo apt-get install g++ unzip zip\n",
        "    !sudo apt autoremove\n",
        "    !sudo apt-get install openjdk-11-jdk\n",
        "    !wget https://github.com/bazelbuild/bazel/releases/download/1.0.0/bazel-1.0.0-installer-linux-x86_64.sh\n",
        "    !chmod +x bazel-1.0.0-installer-linux-x86_64.sh\n",
        "    !./bazel-1.0.0-installer-linux-x86_64.sh --user\n",
        "    os.environ['PATH'] += ':/root/bin'\n",
        "    !pip install tensorflow==1.0.0\n",
        "    !pip install numpy\n",
        "    !pip install nltk\n",
        "    !python -m nltk.downloader punkt\n",
        "\n",
        "    !pip install requests\n",
        "\n",
        "    # download checkpoint (2M iteration)\n",
        "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "    gdd.download_file_from_google_drive(file_id='0B3laN3vvvSD2WWxuR3VRQzhycWM',\n",
        "                                        dest_path='/content/Pretrained-Show-and-Tell-model/model.ckpt-1000000.data-00000-of-00001')\n",
        "    gdd.download_file_from_google_drive(file_id='0B3laN3vvvSD2T1RPeDA5djJ6bFE',\n",
        "                                        dest_path='/content/Pretrained-Show-and-Tell-model/model.ckpt-2000000.data-00000-of-00001')\n",
        "    \n",
        "    # solved mismatch problem of variable name in tensorflow graph\n",
        "    replace(\"/content/img_caption/research/im2txt/im2txt/inference_utils/caption_generator.py\"\n",
        "    , '[:-self.beam_size]', '')\n",
        "\n",
        "    # modify_files()\n",
        "    replace(\"/content/img_caption/research/im2txt/im2txt/run_inference.py\", '      for i, caption in enumerate(captions):', '      f=open(\"/result.txt\", mode=\"w+\", encoding=\"utf-8\")')\n",
        "    replace(\"/content/img_caption/research/im2txt/im2txt/run_inference.py\", '        sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]', '      sentence = [vocab.id_to_word(w) for w in captions[0].sentence[1:-1]]')\n",
        "    replace(\"/content/img_caption/research/im2txt/im2txt/run_inference.py\", '        sentence = \" \".join(sentence)', '      sentence = \" \".join(sentence)\\n      f.write(sentence)\\n      f.close()')\n",
        "    replace(\"/content/img_caption/research/im2txt/im2txt/run_inference.py\", '        print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))', '      print(\"  %d) %s (p=%f)\" % (0, sentence, math.exp(captions[0].logprob)))')\n",
        "\n",
        "\n",
        "setup_img_captioning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/30 18:34:41 d2.config.compat]: \u001b[0mConfig './detectron2_repo/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "model_final_f10217.pkl: 178MB [00:02, 65.1MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUSfG34JgXf4",
        "colab_type": "code",
        "outputId": "cde31269-ceaa-45b5-d46b-c1570f5b607b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "# the code executed when server is running \n",
        "#########################################################################\n",
        "# Object Detection part\n",
        "#########################################################################\n",
        "def inferrence(im, mode='labels'):\n",
        "  outputs = predictor(im)\n",
        "\n",
        "  #this create text labels from int labels\n",
        "  def _create_text_labels(classes, scores, class_names):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          classes (list[int] or None):\n",
        "          scores (list[float] or None):\n",
        "          class_names (list[str] or None):\n",
        "\n",
        "      Returns:\n",
        "          list[str] or None\n",
        "      \"\"\"\n",
        "      # we take items over score_threshold\n",
        "      score_threshold = 0.8\n",
        "      labels = None\n",
        "      if classes is not None and class_names is not None and len(class_names) > 1:\n",
        "          labels = [class_names[i] for i in classes]\n",
        "      if scores is not None:\n",
        "          if labels is None:\n",
        "              labels = [\"{:.0f}%\".format(s * 100) for s in scores]\n",
        "          else:\n",
        "              labels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores) if score_threshold >= 0.8]\n",
        "      return labels\n",
        "\n",
        "  # used like this in 'draw_instance_predictions(self, predictions)' (this 'self' is a Visualizer object)\n",
        "  # scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "  # classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n",
        "  # labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
        "\n",
        "  v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "\n",
        "  # We can use `Visualizer` to draw the predictions on the image.\n",
        "\n",
        "  if mode == 'segmentation':\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
        "  elif mode == 'labels':\n",
        "    # variable 'labels' is text labels included in image\n",
        "    predictions = outputs[\"instances\"].to(\"cpu\")\n",
        "    scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "    classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n",
        "    labels = _create_text_labels(classes, scores, v.metadata.get(\"thing_classes\", None))\n",
        "    labels = [x.split()[0] for x in labels]\n",
        "    labels = list(OrderedDict((element, None) for element in labels))\n",
        "    return labels\n",
        "\n",
        "#########################################################################\n",
        "# TTS part\n",
        "#########################################################################\n",
        "def inferrence_tts(text):\n",
        "  print(\"Audio synthesis with latest record\")\n",
        "  in_fpath = Path(\"audio.wav\")\n",
        "  reprocessed_wav = encoder.preprocess_wav(in_fpath)\n",
        "  original_wav, sampling_rate = librosa.load(in_fpath)\n",
        "  preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n",
        "  embed = encoder.embed_utterance(preprocessed_wav)\n",
        "  print(\"Synthesizing new audio...\")\n",
        "  with io.capture_output() as captured:\n",
        "    specs = synthesizer.synthesize_spectrograms([text], [embed])\n",
        "  generated_wav = vocoder.infer_waveform(specs[0])\n",
        "  generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
        "  # display(Audio(generated_wav, rate=synthesizer.sample_rate))\n",
        "  #librosa.output.write_wav(\"./result.wav\", generated_wav, synthesizer.sample_rate)\n",
        "  import numpy as np\n",
        "  from scipy.io.wavfile import write\n",
        "  write('first_sine_wave.wav', sps, generated_wav)\n",
        "  return\n",
        "\n",
        "#########################################################################\n",
        "# OCR part\n",
        "#########################################################################\n",
        "def inferrence_ocr(im_path):\n",
        "  import sys\n",
        "  print(\"inside inference\")\n",
        "  os.system('tesseract '+im_path+' output')\n",
        "\n",
        "#########################################################################\n",
        "# Image Captioning part\n",
        "#########################################################################\n",
        "import subprocess as sp\n",
        "from requests import get\n",
        "\n",
        "def download_img(url):\n",
        "  %cd /content\n",
        "  download(url,\"hello.jpg\")\n",
        "\n",
        "def download(url, file_name):\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        response = get(url)\n",
        "        file.write(response.content)\n",
        "        file.close()\n",
        "\n",
        "def run_img_captioning():\n",
        "    CHECKPOINT_PATH=\"/content/Pretrained-Show-and-Tell-model/model.ckpt-2000000\";\n",
        "    VOCAB_FILE=\"/content/Pretrained-Show-and-Tell-model/word_counts.txt\";\n",
        "    IMAGE_FILE=\"/content/pics/hello.jpg\";\n",
        "\n",
        "    if os.path.exists('/result.txt'):\n",
        "        os.remove('/result.txt')\n",
        "\n",
        "    %cd /content/img_caption/research/im2txt\n",
        "    !bazel build -c opt im2txt/run_inference\n",
        "\n",
        "    !bazel-bin/im2txt/run_inference \\\n",
        "        --checkpoint_path=$CHECKPOINT_PATH \\\n",
        "        --vocab_file=$VOCAB_FILE \\\n",
        "        --input_files=$IMAGE_FILE\n",
        "\n",
        "    g=open('/result.txt')\n",
        "    line=g.readline()\n",
        "    # print(line)\n",
        "    g.close()\n",
        "    return line\n",
        "\n",
        "#code for actually running server\n",
        "from flask import Flask\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from flask import make_response\n",
        "from flask import send_from_directory\n",
        "import os\n",
        "from flask_ngrok import run_with_ngrok\n",
        "app = Flask(__name__)\n",
        "UPLOAD_FOLDER = './pics'\n",
        "if not os.path.exists(UPLOAD_FOLDER):\n",
        "  os.mkdir(UPLOAD_FOLDER)\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "#server code which handles request for object detection\n",
        "@app.route('/picture', methods=['POST', 'GET'])\n",
        "def upload_file():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        if file:\n",
        "            filename = \"hello.jpg\"\n",
        "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "            im = cv2.imread(\"./pics/hello.jpg\")\n",
        "            result = inferrence(im)\n",
        "            print(result)\n",
        "            text = \"\"\n",
        "            if len(result) == 0:\n",
        "              text = \"Unknown object\"\n",
        "            else:\n",
        "              text = ', '.join(result)\n",
        "            print(text)\n",
        "            return jsonify({'result':text})\n",
        "\n",
        "#server code which handles request for OCR\n",
        "@app.route('/swipeDown', methods=['POST', 'GET'])\n",
        "def upload_f():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        if file:\n",
        "            print(\"in OCR\")\n",
        "            filename = \"hello.jpg\"\n",
        "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "            inferrence_ocr(\"./pics/hello.jpg\")\n",
        "            with open('./output.txt', 'r') as content_file:\n",
        "              content = content_file.read()\n",
        "            text = \"\"\n",
        "            if len(content) == 0:\n",
        "              text = \"text not detected\"\n",
        "            else:\n",
        "              text = content\n",
        "            print(text)\n",
        "        return jsonify({'result':text})\n",
        "\n",
        "#server code which handles request for Image Captioning \n",
        "@app.route('/swipeUp', methods=['POST', 'GET'])\n",
        "def upload_file2():\n",
        "    if request.method == 'POST':\n",
        "      file = request.files['file']\n",
        "      if file:\n",
        "          filename = \"hello.jpg\"\n",
        "          file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "          im = cv2.imread(\"/content/pics/hello.jpg\")\n",
        "          result = run_img_captioning()\n",
        "          print(result)\n",
        "          text = \"\"\n",
        "          if len(result) == 0:\n",
        "            text = \"Unknown object\"\n",
        "          else:\n",
        "            text = result\n",
        "          print(text)\n",
        "          return jsonify({'result':text})\n",
        "\n",
        "app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://87f0804b.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "/content/img_caption/research/im2txt\n",
            "\u001b[32mINFO: \u001b[0mWriting tracer profile to '/root/.cache/bazel/_bazel_root/b8b3f7b8f4e892337dbf031ef12d7650/command.profile.gz'\n",
            "\u001b[32mAnalyzing:\u001b[0m target //im2txt:run_inference (0 packages loaded, 0 targets configu\\\n",
            "\u001b[32mINFO: \u001b[0mAnalyzed target //im2txt:run_inference (0 packages loaded, 0 targets configured).\n",
            "\u001b[32mINFO: \u001b[0mFound 1 target...\n",
            "Target //im2txt:run_inference up-to-date:\n",
            "  bazel-bin/im2txt/run_inference\n",
            "\u001b[32mINFO: \u001b[0mElapsed time: 0.195s, Critical Path: 0.00s\n",
            "\u001b[32mINFO: \u001b[0m0 processes.\n",
            "\u001b[32mINFO:\u001b[0m Build completed successfully, 1 total action\n",
            "\u001b[0m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "INFO:tensorflow:Building model.\n",
            "INFO:tensorflow:Initializing vocabulary from file: /content/Pretrained-Show-and-Tell-model/word_counts.txt\n",
            "INFO:tensorflow:Created vocabulary with 11520 words\n",
            "INFO:tensorflow:Running caption generation on 1 files matching /content/pics/hello.jpg\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "INFO:tensorflow:Loading model from checkpoint: /content/Pretrained-Show-and-Tell-model/model.ckpt-2000000\n",
            "INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-2000000\n",
            "Captions for image hello.jpg:\n",
            "  0) a group of people walking down a street . (p=0.000026)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Nov/2019 18:58:33] \"\u001b[37mPOST /swipeUp HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a group of people walking down a street .\n",
            "a group of people walking down a street .\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}